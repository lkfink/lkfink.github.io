---
layout: page
title: Research
description: Lauren Fink's research projects
---
<HEAD>
<!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-114823830-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-114823830-1');
  </script>
</HEAD>

<div class="navbar">
    <div class="navbar-inner">
        <ul class="nav">
            <li><a href="#Modeling attention">Modeling attention</a></li>
            <li><a href="#GEM">GEM</a></li> 
            <li><a href="#Mobile eye-tracking">Mobile eye-tracking</a></li>
            <li><a href="#musicET">MET</a></li> 
            <li><a href="#WAC">WAC</a></li> 
        </ul>
    </div>
</div>



### <a name="Modeling attention"></a>Modeling attention to music
In this project, I use a combination of computational modeling, psychophysics, eye-tracking, and electroencephalography (EEG) to measure and predict dynamic attention to auditory stimuli. Specifically, I aim to 1) assess the potential of a stimulus-driven [linear oscillator model](http://atonal.ucdavis.edu/projects/musical_spaces/rhythm/btb/){:target="_blank"} to predict attention to complex musical stimuli and 2) determine the relationship between ocular and cortical responses to auditory rhythms and whether pupil dynamics can index auditory attention in a manner similar to EEG signatures. 

<!--

<iframe width="560" height="315" src="https://www.youtube.com/embed/OMVWDExIq38" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

This video shows oscillations in pupil size while listening to a clip of music. The video is sped up 5x.

-->

##### Papers related to this project:  

> **Fink, L.**, Hurley, B., Geng, J. & Janata, P. (2018). A linear oscillator model predicts dynamic temporal attention and pupillary sensorimotor synchronization to rhythmic musical patterns.  *Journal of Eye Movement Research, 11*(2):12. [DOI: 10.16910/jemr.11.2.12](https://bop.unibe.ch/JEMR/article/view/4285/){:target="_blank"}.  

> Hurley, B., **Fink, L.**, & Janata, P. (2018). Mapping the dynamic allocation of attention in musical patterns. *Journal of Experimental Psychology: Human Perception & Performance.* Advance online publication. [DOI: 10.1037/xhp0000563](http://psycnet.apa.org/doiLanding?doi=10.1037%2Fxhp0000563){:target="_blank"}  

##### Recent conference presentation about an extension of this project: 
> In this video I discuss the possibility of predicting listeners' absorption into music from their pupils. 
> <iframe width="560" height="315" src="https://www.youtube.com/embed/5bpDhrxUvLg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<br><br>

### <a name="GEM"></a>Groove Enhancement Machine (GEM)
This project involves a real-time adaptive music making device to enhance motor synchrony and subjective enjoyment between groups of people tapping together. Our paper about this project is published in [Frontiers in Human Neuroscience](https://doi.org/10.3389/fnhum.2022.916551){:target="_blank"}. All code and hardware specifications required to build the device are publicly available. 

> **Fink, L.K.**, Alexander, P.C., & Janata, P. (2022). The Groove Enhancement Machine (GEM): A multi-person adaptive metronome to manipulate sensorimotor synchronization and subjective enjoyment. *Front. Hum. Neurosci. 16*:916551. [doi: 10.3389/fnhum.2022.916551](https://doi.org/10.3389/fnhum.2022.916551){:target="_blank"}.

This short documentary, directed by Joerg Altekruse, highlights an early prototype of the project, as well as related work in the Janata Lab.

<iframe title="Groove-Maschine" allowfullscreen="true" style="transition-duration:0;transition-property:no;margin:0 auto;position:relative;display:block;background-color:#000000;" frameborder="0" scrolling="no" width="720" height="406" src="https://www.arte.tv/player/v3/index.php?json_url=https%3A%2F%2Fapi.arte.tv%2Fapi%2Fplayer%2Fv1%2Fconfig%2Fde%2F074208-005-A%3Fautostart%3D0%26lifeCycle%3D1&amp;lang=de_DE&amp;mute=0"></iframe>

Below is a recent talk given at the Rhythm, Perception & Production Workshop (2021) which provides a more up-to-date and scientific overview of the project: 
> <iframe width="560" height="315" src="https://www.youtube.com/embed/optqIxLbz2k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<!-- ![groove enhancement machine collage]({{ BASE_PATH }}/assets/publpics/gem_example.png)  -->

<br><br>

### <a name="Mobile eye-tracking"></a>Mobile eye-tracking

I am currently using [Pupil Labs'](https://pupil-labs.com/){:target="_blank"} mobile eye-tracking glasses in a variety of music/neuroscience projects. <!-- For anyone considering purchasing from Pupil Labs, I previously wrote up a [short guide of considerations](http://lkfink.github.io/pages/PupilLabs_aBeginnersGuide.html){:target="_blank"}. -->
 
![pupil labs collage]({{ BASE_PATH }}/assets/publpics/puplabs_collage.png) 

For example, in a recent pilot study, we asked whether the eye movement patterns of pianists performing from memory are consistent across mutliple performances on different days. Basically, we wanted to know if eye movements become a (subconsciously) rehearsed and stable aspect of the full body memorized musical performance. You can listen to me talk about our prelimary results here! 

<iframe title="vimeo-player" src="https://player.vimeo.com/video/728534533?h=8140ab1cf3" width="640" height="360" frameborder="0" allowfullscreen></iframe>


<!--
At the [Max Planck Institute for Empirical Aesthetics](https://www.aesthetics.mpg.de/en.html){:target="_blank"}, I, together with [Elke Lange](https://www.aesthetics.mpg.de/en/the-institute/people/lange.html){:target="_blank"}, would like to bring mobile eye-tracking into the concert hall ([ArtLab](https://www.aesthetics.mpg.de/index.php?id=625&L=1){:target="_blank"}). While it may seem odd to use eye-tracking (vision) in a musical (auditory) context, a growing number of laboratory studies have clearly established that sound systematically affects visual processing. However, eye-tracking has yet to be widely adopted in auditory, and more specifically, musical, contexts. Previously, mobile eye-trackers have been used to examine gaze patterns of up to 3 musicians playing together; however, no studies have examined mobile eye-tracking data from multiple audience members at a concert, in addition to the musicians on stage. Such an undertaking would allow for comparisons of:
-	gaze location (what audience members and what musicians are looking at)
-	eye movement dynamics (when, in relation to musical events, and each other, audience members and musicians shift their gaze)
-	blink rate (how often people blink) and blink timing (when people blink)

These measures will allow us to answer a number of questions about how and when music affects ocular dynamics. On a lower level, we can also answer questions about which neural mechanisms may underlie certain aspects of musical processing, as many ocular measures have well established neural substrates. Further, we can pit auditory and visual saliency against each other to answer questions related to cross-modal interactions.  
--> 

<br><br>

### <a name="musicET"></a>Conference on Music and Eye-Tracking 

[![METimage](../../assets/publpics/METimage.png)](https://lkfink.github.io/pages/publpics/METimage.html){:target="_blank"} 


In August 2017 and July 2022, [Elke Lange](https://www.aesthetics.mpg.de/en/the-institute/people/lange.html){:target="_blank"} and I organized the [Conference on Music & Eye-Tracking](https://www.ae.mpg.de/musicET){:target="_blank"}, which was held at the Max Planck Institute for Empirical Aesthetics in Frankfurt, Germany. <br/>  

2017 Conference Program: [![Click here to view the MET conference program](icons16/pdf-icon.png)]({{ BASE_PATH }}/assets/MET17-KonferenzbroschuÌˆre.pdf){:target="_blank"}   <br/>  

A [Special Issue on Music and Eye-Tracking](https://bop.unibe.ch/JEMR/issue/view/793){:target="_blank"} with selected presentations from the conference was published in the Journal of Eye Movement Research.  

> **Fink, L.**, Lange, E. & Groner, R. (2019). The application of eye-tracking in music research. *Journal of
Eye Movement Research, 11(2)*:1. DOI: 10.16910/jemr.11.2.1.

2022 Conference Program: [![Click here to view the MET conference program](icons16/pdf-icon.png)](https://indico.aesthetics.mpg.de/event/2/attachments/26/30/Lange-Fink-MusicET2022-Full-Program-DIGITAL.pdf){:target="_blank"}   <br/> 



<br><br>

### <a name="WAC"></a>Writing Across the Curriculum (WAC)
I served for three years as the lead Graduate Writing Fellow in the University of California Davis's Writing Across the Curriculum program. In this role, I managed a team of six Graduate Writing Fellows and acted as a liason between graduate fellows and faculty. The WAC fellows hold one-on-one [writing consultations](http://writing.ucdavis.edu/wac/consultations){:target="_blank"} with graduate students and postdocs from any discipline, offer writing [retreats](http://writing.ucdavis.edu/wac/retreats){:target="_blank"} and [workshops](http://writing.ucdavis.edu/wac/workshops){:target="_blank"}, as well as specialized programs such as the [Graduate Certificate in Writing Theory and Practice](http://writing.ucdavis.edu/wac/certificate){:target="_blank"} and the [Writing Partner Program](http://writing.ucdavis.edu/wac/resources/writing-partner-program){:target="_blank"}. I developed the curriculum for the Graduate Certificate in Writing Theory and Practice, along with WAC peers [Tori White and Julia Ribeiro](http://writing.ucdavis.edu/wac/gfw-bios){:target="_blank"}. Our reflections and insights as a graduate writing consultant team have recently been published in Praxis:

> Wittstock, S., Sperber, L., Kirk, G., McCarty, K., de Sola-Smith, K., Wade, J., Simon, M., **Fink, L.** (2022). Making what we know explicit: Perspectives from graduate writing consultants on supporting graduate writers. *Praxis: A Writing Center Journal, 19*(2). https://www.praxisuwc.com/192-wittstock-et-al. 


I also conducted numerous research projects which were presented at the following conferences: 

> Mikovits, M., Sperber, L., **Fink, L.** & Prebel, J. (2019, March). Writing Fellows as Agents of Transfer: Training in Threshold Concepts to Support Campus-Wide Sites of Writing. *Symposium presented at the College Composition and Communication Convention, Pittsburgh, PA.*  

> **Fink, L.**, Ribeiro, J., & White, V. (2018, March). Transforming graduate writing experiences: A new Writing Across the Curriculum (WAC) certificate program. *Symposium presented at the College Composition and Communication Convention, Kansas City, MO.*  

> Bright, A., Singleton, J., **Fink, L.**, & Rodger, K. (2017, March). Cultivating a Rhetorical Consciousness: Supporting Graduate Student Writers Across the Curriculum. *Symposium presented at the College Composition and Communication Convention, Portland, OR.*  

> **Fink, L.** & Rodger, K. (2016, June). Mapping neuroscience through professional writing. *Talk presented at the International Writing Across the Curriculum Conference, Ann Arbor, MI.*

Most recently, we have had an article about our collective knowledge as graduate writing consultants accepted for publication! 

> Wittstock, S., Sperber, L., Kirk, G., McCarty, K., de Sola-Smith, K., Wade, J., Simon, M., **Fink, L.** (2021, in press). Making what we know explicit: Perspectives from graduate writing consultants on supporting graduate writers. *Praxis: A Writing Center Journal.*






